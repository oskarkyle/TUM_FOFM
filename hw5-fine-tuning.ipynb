{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!mamba install --force-reinstall aiohttp -y\n!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T12:25:52.313480Z","iopub.execute_input":"2024-06-13T12:25:52.313942Z","iopub.status.idle":"2024-06-13T12:30:44.123284Z","shell.execute_reply.started":"2024-06-13T12:25:52.313906Z","shell.execute_reply":"2024-06-13T12:30:44.121918Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TextStreamer","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:30:44.126155Z","iopub.execute_input":"2024-06-13T12:30:44.126505Z","iopub.status.idle":"2024-06-13T12:31:06.926225Z","shell.execute_reply.started":"2024-06-13T12:30:44.126466Z","shell.execute_reply":"2024-06-13T12:31:06.925412Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2024-06-13 12:30:55.424548: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-13 12:30:55.424648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-13 12:30:55.574475: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\nFastLanguageModel.for_inference(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:31:06.927295Z","iopub.execute_input":"2024-06-13T12:31:06.927860Z","iopub.status.idle":"2024-06-13T12:31:43.508219Z","shell.execute_reply.started":"2024-06-13T12:31:06.927834Z","shell.execute_reply":"2024-06-13T12:31:43.507353Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5cd363c16964df59594995da31986d3"}},"metadata":{}},{"name":"stdout","text":"==((====))==  Unsloth: Fast Llama patching release 2024.6\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae9593681a443b1943bd2393d05e912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a32dbb117cb4eac9fb4bc5278ac68ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b76bace6e1644d77821bee04afd6f677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62e6e9f5cbf04265b27be2df6e9e7668"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/464 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cfd08ec18dc4edb8640b8f4b5f728b8"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"# initialize LoRA parameters and associate them with the model\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                       \"gate_proj\",\"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\", # reducing memory\n    random_state = 1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:31:43.509537Z","iopub.execute_input":"2024-06-13T12:31:43.509847Z","iopub.status.idle":"2024-06-13T12:31:47.599047Z","shell.execute_reply.started":"2024-06-13T12:31:43.509820Z","shell.execute_reply":"2024-06-13T12:31:47.597823Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"# load OpenAI's grade-school-math dataset from the paper Cobbe et al, 'Training Verifiers to Solve Math World Problems', 2021\ndataset = load_dataset(\"qwedsacf/grade-school-math-instructions\", split = \"train\")\nprint('The dataset has ', len(dataset), '  many entries')\n\ndataset = dataset.select(range(500)) #Take only 500 examples\nprint('We take ' , len(dataset), ' many entries for this homework ')\n\nprint('This is how an entry of the dataset looks like: ' , dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:31:47.603190Z","iopub.execute_input":"2024-06-13T12:31:47.603564Z","iopub.status.idle":"2024-06-13T12:31:50.149984Z","shell.execute_reply.started":"2024-06-13T12:31:47.603535Z","shell.execute_reply":"2024-06-13T12:31:50.148908Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/852 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff2c9f193c83496c978b12c2f724288c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.55M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94ab7bbe86fd4dababb2e60c746cde57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8792 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dcdd82700bc4e4b997777622284897b"}},"metadata":{}},{"name":"stdout","text":"The dataset has  8792   many entries\nWe take  500  many entries for this homework \nThis is how an entry of the dataset looks like:  {'INSTRUCTION': 'This math problem has got me stumped: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\\nCan you show me the way?', 'RESPONSE': 'Natalia sold 48/2 = 24 clips in May.\\nNatalia sold 48+24 = 72 clips altogether in April and May.', 'SOURCE': 'grade-school-math'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Prepare the data for finetuning, we concatenate the INSTRUCTION and RESPONSE fields from the grade-school-math instructions dataset\n# into one string.\n\nprompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts(examples):\n    instructions = examples[\"INSTRUCTION\"]\n    responses     = examples[\"RESPONSE\"]\n    texts = []\n    for instruction, response in zip(instructions, responses):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = prompt.format(instruction, response) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\n\ndataset = dataset.map(formatting_prompts, batched = True)\nprint(dataset[0]['text'])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:31:50.151495Z","iopub.execute_input":"2024-06-13T12:31:50.152283Z","iopub.status.idle":"2024-06-13T12:31:50.218827Z","shell.execute_reply.started":"2024-06-13T12:31:50.152242Z","shell.execute_reply":"2024-06-13T12:31:50.217802Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbefb49e275c49b4a1f3a5cfedf01fa3"}},"metadata":{}},{"name":"stdout","text":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nThis math problem has got me stumped: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\nCan you show me the way?\n\n### Response:\nNatalia sold 48/2 = 24 clips in May.\nNatalia sold 48+24 = 72 clips altogether in April and May.<|end_of_text|>\n","output_type":"stream"}]},{"cell_type":"code","source":"#Here we check how the pretrained model responds to 3 simple questions before finetuning.\nQ1 = \"I have 10 apples, my brother took half of them from me, I lost 1, and my friend gave me 3. How many do I have now?\"\nQ2 = \"I earn five euros per hour. I worked two hours yesterday and five hours today. How much did I earn in total?\"\nQ3 = \"In year 2000 I was 20 years old. My sister is 5 years younger than me. How old is she in 2020?\"\n\ninput1 = tokenizer([prompt.format(Q1, \"\",)],return_tensors = \"pt\").to(\"cuda\")\ninput2 = tokenizer([prompt.format(Q2, \"\",)],return_tensors = \"pt\").to(\"cuda\")\ninput3 = tokenizer([prompt.format(Q3, \"\",)],return_tensors = \"pt\").to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:31:50.220260Z","iopub.execute_input":"2024-06-13T12:31:50.221027Z","iopub.status.idle":"2024-06-13T12:31:50.480462Z","shell.execute_reply.started":"2024-06-13T12:31:50.220987Z","shell.execute_reply":"2024-06-13T12:31:50.479525Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Response to first question\n_= model.generate(**input1, streamer = TextStreamer(tokenizer), max_new_tokens = 70, do_sample=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:31:50.481671Z","iopub.execute_input":"2024-06-13T12:31:50.481997Z","iopub.status.idle":"2024-06-13T12:31:59.794603Z","shell.execute_reply.started":"2024-06-13T12:31:50.481947Z","shell.execute_reply":"2024-06-13T12:31:59.793705Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nI have 10 apples, my brother took half of them from me, I lost 1, and my friend gave me 3. How many do I have now?\n\n### Response:\nI have 10 apples, my brother took half of them from me, I lost 1, and my friend gave me 3. How many do I have now?\n\n### Explanation:\nI have 10 apples, my brother took half of them from me, I lost 1, and my friend gave me 3. How many do I\n","output_type":"stream"}]},{"cell_type":"code","source":"#Response to second question\n_= model.generate(**input2, streamer = TextStreamer(tokenizer), max_new_tokens = 70, do_sample=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:31:59.795828Z","iopub.execute_input":"2024-06-13T12:31:59.796213Z","iopub.status.idle":"2024-06-13T12:32:00.616177Z","shell.execute_reply.started":"2024-06-13T12:31:59.796181Z","shell.execute_reply":"2024-06-13T12:32:00.615040Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nI earn five euros per hour. I worked two hours yesterday and five hours today. How much did I earn in total?\n\n### Response:\nI earned 15 euros in total.<|end_of_text|>\n","output_type":"stream"}]},{"cell_type":"code","source":"#Response to third question\n_= model.generate(**input3, streamer = TextStreamer(tokenizer), max_new_tokens = 70, do_sample=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:32:00.617891Z","iopub.execute_input":"2024-06-13T12:32:00.618805Z","iopub.status.idle":"2024-06-13T12:32:05.832482Z","shell.execute_reply.started":"2024-06-13T12:32:00.618764Z","shell.execute_reply":"2024-06-13T12:32:05.831634Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nIn year 2000 I was 20 years old. My sister is 5 years younger than me. How old is she in 2020?\n\n### Response:\nIn year 2000 I was 20 years old. My sister is 5 years younger than me. How old is she in 2020?\n\n### Explanation:\nIn year 2000 I was 20 years old. My sister is 5 years younger than me. How old is she in 2020?\n\n### Instruction:\nIn year\n","output_type":"stream"}]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, \n    args = TrainingArguments(\n        per_device_train_batch_size = 4,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        num_train_epochs = 1,\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:33:24.930045Z","iopub.execute_input":"2024-06-13T12:33:24.930810Z","iopub.status.idle":"2024-06-13T12:33:25.220178Z","shell.execute_reply.started":"2024-06-13T12:33:24.930761Z","shell.execute_reply":"2024-06-13T12:33:25.219303Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:33:33.162343Z","iopub.execute_input":"2024-06-13T12:33:33.162721Z","iopub.status.idle":"2024-06-13T12:33:33.169992Z","shell.execute_reply.started":"2024-06-13T12:33:33.162695Z","shell.execute_reply":"2024-06-13T12:33:33.168834Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.748 GB.\n5.922 GB of memory reserved.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:33:36.220468Z","iopub.execute_input":"2024-06-13T12:33:36.221601Z","iopub.status.idle":"2024-06-13T12:39:19.770156Z","shell.execute_reply.started":"2024-06-13T12:33:36.221552Z","shell.execute_reply":"2024-06-13T12:39:19.769162Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 16 | Total steps = 31\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [31/31 05:29, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.997600</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.993000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.906200</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.791800</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.691000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.594100</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.337200</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.350100</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.214100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.199500</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.244800</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.225600</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.141500</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.100900</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.130000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.155700</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.162200</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.094500</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.114900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.104000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.976500</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.046500</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.040200</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.018000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.106700</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.110600</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.003000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.990400</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.055500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.082400</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.035700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"import os\ncurrent_path = os.getcwd()\nmodel.save_pretrained(\"lora_model\") \nprint(f\"Address: {current_path}/lora_model\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:42:11.666273Z","iopub.execute_input":"2024-06-13T12:42:11.667000Z","iopub.status.idle":"2024-06-13T12:42:12.314042Z","shell.execute_reply.started":"2024-06-13T12:42:11.666966Z","shell.execute_reply":"2024-06-13T12:42:12.312912Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Address: /kaggle/working/lora_model\n","output_type":"stream"}]},{"cell_type":"code","source":"if True:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\",\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:42:15.330506Z","iopub.execute_input":"2024-06-13T12:42:15.331312Z","iopub.status.idle":"2024-06-13T12:42:23.593008Z","shell.execute_reply.started":"2024-06-13T12:42:15.331274Z","shell.execute_reply":"2024-06-13T12:42:23.592191Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth: Fast Llama patching release 2024.6\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"#Response to first question after finetuning\n_= model.generate(**input1, streamer = TextStreamer(tokenizer), max_new_tokens = 70, do_sample=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:42:26.871763Z","iopub.execute_input":"2024-06-13T12:42:26.872665Z","iopub.status.idle":"2024-06-13T12:42:30.930034Z","shell.execute_reply.started":"2024-06-13T12:42:26.872619Z","shell.execute_reply":"2024-06-13T12:42:30.928765Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nI have 10 apples, my brother took half of them from me, I lost 1, and my friend gave me 3. How many do I have now?\n\n### Response:\nI have 10 apples - half of them = 10/2 = 5\nI have 5 apples - 1 = 5-1 = 4\nI have 4 apples + 3 = 4+3 = 7<|end_of_text|>\n","output_type":"stream"}]},{"cell_type":"code","source":"#Response to second question after finetuning\n_= model.generate(**input2, streamer = TextStreamer(tokenizer), max_new_tokens = 70, do_sample=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:42:33.513813Z","iopub.execute_input":"2024-06-13T12:42:33.514801Z","iopub.status.idle":"2024-06-13T12:42:37.754167Z","shell.execute_reply.started":"2024-06-13T12:42:33.514749Z","shell.execute_reply":"2024-06-13T12:42:37.753098Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nI earn five euros per hour. I worked two hours yesterday and five hours today. How much did I earn in total?\n\n### Response:\nI earned 5 euros per hour * 2 hours = 10 euros yesterday.\nI earned 5 euros per hour * 5 hours = 25 euros today.\nI earned 10 euros yesterday + 25 euros today = 35 euros in total.<|end_of_text|>\n","output_type":"stream"}]},{"cell_type":"code","source":"#Response to third question after finetuning\n_= model.generate(**input3, streamer = TextStreamer(tokenizer), max_new_tokens = 70, do_sample=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:43:01.991045Z","iopub.execute_input":"2024-06-13T12:43:01.991402Z","iopub.status.idle":"2024-06-13T12:43:05.196868Z","shell.execute_reply.started":"2024-06-13T12:43:01.991376Z","shell.execute_reply":"2024-06-13T12:43:05.196011Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nIn year 2000 I was 20 years old. My sister is 5 years younger than me. How old is she in 2020?\n\n### Response:\nIn 2020 I will be 20+20=40 years old.\nMy sister will be 5 years younger than me, so she will be 40-5=35 years old.<|end_of_text|>\n","output_type":"stream"}]}]}