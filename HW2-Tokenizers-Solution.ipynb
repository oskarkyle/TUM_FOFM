{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22eede02",
   "metadata": {},
   "source": [
    "# Homework 2 - Solution\n",
    "\n",
    "Implement the train, decode, and encode functions of the tokenizer below.\n",
    "The tokenizer should use the GPT2_SPLIT_PATTERN and treat the special token <|endoftext|> appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eeea6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Base class for Tokenizers\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        assert vocab_size >= 256\n",
    "        # default: vocab size of 256 (all bytes), no merges, no patterns\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.pattern = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\" # GPT2_SPLIT_PATTERN\n",
    "        self.special_tokens = {'<|endoftext|>': vocab_size}\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = self._build_vocab() # int -> bytes\n",
    "\n",
    "    def train(self, text, verbose=False):\n",
    "        # Tokenizer can train a vocabulary of size vocab_size from text\n",
    "        \n",
    "        num_merges = self.vocab_size - 256\n",
    "        \n",
    "        split_text = re.findall(self.pattern, text) # GPT2_split_pattern\n",
    "        tokens = [list(ch.encode(\"utf-8\")) for ch in split_text]        \n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            # count the number of times every consecutive pair appears\n",
    "            counts = {}\n",
    "            for splits in tokens:\n",
    "                # passing in counts will update it in place, adding up the counts\n",
    "                self._get_frequencies(splits, counts)\n",
    "            # find the pair with the highest count\n",
    "            pair = max(counts, key=counts.get)\n",
    "            # assign a new token to the next available id\n",
    "            idx = 256 + i\n",
    "            # replace all occurrences of pair in tokens with idx\n",
    "            tokens = [self._merge(splits, pair, idx) for splits in tokens]\n",
    "            # save the merge\n",
    "            self.merges[pair] = idx\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({self.vocab[idx]}) had {counts[pair]} occurrences\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Unlike encode_ordinary, this function handles special tokens.\n",
    "        \"\"\"\n",
    "        # encode the user desire w.r.t. handling of special tokens\n",
    "        special = self.special_tokens\n",
    "\n",
    "        # we handle special tokens by splitting the text\n",
    "        # based on the occurrence of any exact match with any of the special tokens\n",
    "        # we can use re.split for this. note that surrounding the pattern with ()\n",
    "        # makes it into a capturing group, so the special tokens will be included\n",
    "        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "        special_chunks = re.split(special_pattern, text)\n",
    "        # now all the special characters are separated from the rest of the text\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for part in special_chunks:\n",
    "            if part in special:\n",
    "                # this is a special token, encode it separately as a special case\n",
    "                ids.append(special[part])\n",
    "            else:\n",
    "                # this is an ordinary sequence, encode it normally\n",
    "                ids.extend(self._encode_ordinary(part))\n",
    "        return ids\n",
    "\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # given ids (list of integers), return Python string\n",
    "        part_bytes = []\n",
    "        for idx in ids:\n",
    "            if idx in self.vocab:\n",
    "                part_bytes.append(self.vocab[idx])\n",
    "            else:\n",
    "                raise ValueError(f\"invalid token id: {idx}\")\n",
    "        text_bytes = b\"\".join(part_bytes)\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        # vocab is simply and deterministically derived from merges\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab\n",
    "    \n",
    "    def _get_frequencies(self,seq, counts=None):\n",
    "        counts = {} if counts is None else counts\n",
    "        for pair in zip(seq,seq[1:]): # iterate over consecutive elements\n",
    "            counts[pair] = counts.get(pair,0) + 1\n",
    "        return counts\n",
    "    \n",
    "    def _merge(self,seq,pair,index):\n",
    "        new_seq = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if seq[i:i+2] == list(pair) and i < len(seq) - 1:\n",
    "                new_seq.append(index)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_seq.append(seq[i])\n",
    "                i += 1\n",
    "        return new_seq\n",
    "    \n",
    "    def _encode_chunk(self, text_bytes):\n",
    "        # return the token ids\n",
    "        # let's begin. first, convert all bytes to integers in range 0..255\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >= 2:\n",
    "            # find the pair with the lowest merge index\n",
    "            stats = self._get_frequencies(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            # subtle: if there are no more merges available, the key will\n",
    "            # result in an inf for every single pair, and the min will be\n",
    "            # just the first pair in the list, arbitrarily\n",
    "            # we can detect this terminating case by a membership check\n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged anymore\n",
    "            # otherwise let's merge the best pair (lowest merge index)\n",
    "            idx = self.merges[pair]\n",
    "            ids = self._merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def _encode_ordinary(self, text):\n",
    "        \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
    "        # split text into chunks of text by categories defined \n",
    "        text_chunks = re.findall(self.pattern, text)\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for chunk in text_chunks:\n",
    "            chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n",
    "            chunk_ids = self._encode_chunk(chunk_bytes)\n",
    "            ids.extend(chunk_ids)\n",
    "        return ids    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7669ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = \"Byte pair encoding[1][2] (also known as digram coding)[3] is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1377e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/14: (105, 110) -> 256 (b'in') had 8 occurrences\n",
      "merge 2/14: (256, 103) -> 257 (b'ing') had 5 occurrences\n",
      "merge 3/14: (111, 100) -> 258 (b'od') had 4 occurrences\n",
      "merge 4/14: (111, 114) -> 259 (b'or') had 4 occurrences\n",
      "merge 5/14: (32, 102) -> 260 (b' f') had 4 occurrences\n",
      "merge 6/14: (99, 258) -> 261 (b'cod') had 3 occurrences\n",
      "merge 7/14: (261, 257) -> 262 (b'coding') had 3 occurrences\n",
      "merge 8/14: (32, 97) -> 263 (b' a') had 3 occurrences\n",
      "merge 9/14: (32, 100) -> 264 (b' d') had 3 occurrences\n",
      "merge 10/14: (115, 116) -> 265 (b'st') had 3 occurrences\n",
      "merge 11/14: (32, 256) -> 266 (b' in') had 3 occurrences\n",
      "merge 12/14: (260, 259) -> 267 (b' for') had 3 occurrences\n",
      "merge 13/14: (116, 101) -> 268 (b'te') had 2 occurrences\n",
      "merge 14/14: (105, 114) -> 269 (b'ir') had 2 occurrences\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(270)\n",
    "\n",
    "tokenizer.train(training_text, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937c161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{270: b'<|endoftext|>',\n",
       " 256: b'in',\n",
       " 257: b'ing',\n",
       " 258: b'od',\n",
       " 259: b'or',\n",
       " 260: b' f',\n",
       " 261: b'cod',\n",
       " 262: b'coding',\n",
       " 263: b' a',\n",
       " 264: b' d',\n",
       " 265: b'st',\n",
       " 266: b' in',\n",
       " 267: b' for',\n",
       " 268: b'te',\n",
       " 269: b'ir'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(tokenizer.vocab.items())[256:]) #the new learnt vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd08c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Byte pair encoding[1][2] (also known as digram coding)[3] is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(training_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
